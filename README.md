# **Aritificial Intelligence Lab 7 Submission**

### Problem 1 :

**Read the reference on MENACE by Michie and check for its implementations.  Pick the one that you like the most and go through the code carefully.  Highlight the parts that you feel are crucial.  If possible, try to code the MENACE in any programming language of your liking.**

### Problem 2:

**Consider a binary bandit with two rewards {1-success, 0-failure}.  The bandit returns 1 or 0 for the action that you select, i.e. 1 or 2.  The rewards are stochastic (but stationary).  Use an epsilon-greedy algorithm discussed in class and decide upon the action to take for maximizing the expected reward.  There are two binary bandits named binaryBanditA.m and binaryBanditB.m are waiting for you**

### Problem 3:

Develop a 10-armed bandit in which all ten mean-rewards start out equal and then take independent random walks (by adding a normally distributed increment with mean zero and standard deviation 0.01 to all mean-rewards on each time step).

{function [value] = bandit_nonstat(action)}

### Problem 4:

 The 10-armed bandit that you developed (bandit_nonstat) is difficult to crack with a standard epsilon-greedy algorithm since the rewards are non-stationary.  We did discuss how to track non-stationary rewards in class.  Write a modified epsilon-greedy agent and show whether it is able to latch onto correct actions or not.  (Try at least 10000 time steps before commenting on results)

## Contributors:

| Tejas Nitin Pakhale     | 202211061      |
| :---------------------- | -------------- |
| **Rajat Kumar Thakur** | **202211070** |
| **Tanay Patel**        | **202211094** |
| **Abhi Tundiya**       | **202211095** |
